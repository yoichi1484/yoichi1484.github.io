---
---

@article{yano-etal-2025-lamdagent,
  title={LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents},
  author       = {Taro Yano and
                  Yoichi Ishibashi and
                  Masafumi Oyamada},
  journal      = {EMNLP},
  volume       = {abs/2505.21963},
  year         = {2025},
  abstract = "Large Language Models (LLMs) excel across diverse tasks, with post-training methods like Supervised Fine-Tuning (SFT), Preference Learning, and Model Merging enabling effective domain and task adaptation. While outcomes can vary with data orderings or component combinations, yet manual pipeline optimization is costly and labor-intensive. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging parameters. We propose LaMDAgent, an LLM Agent-driven framework that autonomously constructs and optimizes end-to-end post-training pipelines by exploring various model improving methods, objects, and their applied orderings based on task-based feedback. LaMDAgent achieves a 9.0-point gain in tool-use accuracy without degrading instruction-following, and identifies high-performing strategies overlooked by manual design.We further analyze the impact of data and model scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.",
  pdf          = {https://arxiv.org/abs/2505.21963},
  preview={paper_2025b.png},
  abbr={EMNLP},
  selected={true}
}

@article{ishibashi2025mining,
  title={Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning},
  author={Ishibashi, Yoichi and Yano, Taro and Oyamada, Masafumi},
  journal      = {arXiv},
  volume       = {abs/2505.10182},
  year         = {2025},
  pdf          = {https://arxiv.org/abs/2505.10182},
  abstract ="Large Language Models (LLMs) have demonstrated significant improvements in reasoning capabilities through supervised fine-tuning and reinforcement learning. However, when training reasoning models, these approaches are primarily applicable to specific domains such as mathematics and programming, which imposes fundamental constraints on the breadth and scalability of training data. In contrast, continual pretraining (CPT) offers the advantage of not requiring task-specific signals. Nevertheless, how to effectively synthesize training data for reasoning and how such data affect a wide range of domains remain largely unexplored. This study provides a detailed evaluation of Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden thought processes underlying texts, based on the premise that texts are the result of the author's thinking process. Specifically, we apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis reveals that Reasoning CPT consistently improves performance across all evaluated domains. Notably, reasoning skills acquired in one domain transfer effectively to others; the performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Furthermore, models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty.",
  preview={paper_2025a.jpg},
  abbr={arXiv},
  selected={true}
}

@article{ishibashi2024can,
  title={Can Large Language Models Invent Algorithms to Improve Themselves?},
  author       = {Yoichi Ishibashi and
                  Taro Yano and
                  Masafumi Oyamada},
  journal      = {NAACL},
  volume       = {abs/2410.15639},
  year         = {2025},
  abstract = "Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5\% in terms of Pass@1 accuracy.",
  pdf          = {https://arxiv.org/abs/2410.15639},
  preview={paper_2024b.jpg},
  abbr={NAACL},
  selected={true}
}

@article{ishibashi2024self,
  title={Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization},
  author       = {Yoichi Ishibashi and
                  Nishimura Yoshimasa},
  journal      = {arXiv},
  volume       = {abs/2404.02183},
  year         = {2024},
  pdf          = {https://arxiv.org/abs/2404.02183},
  code = {https://github.com/tsukushiAI/self-organized-agent},
  preview={paper_2024a.jpg},
  abbr={arXiv},
  selected={true}
}


@article{DBLP:journals/corr/abs-2309-11852,
  author       = {Yoichi Ishibashi and
                  Hidetoshi Shimodaira},
  title        = {Knowledge Sanitization of Large Language Models},
  journal      = {arXiv},
  volume       = {abs/2309.11852},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.11852},
  doi          = {10.48550/ARXIV.2309.11852},
  eprinttype    = {arXiv},
  eprint       = {2309.11852},
  timestamp    = {Mon, 25 Sep 2023 15:34:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-11852.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = {https://arxiv.org/abs/2309.11852},
  code = {https://github.com/yoichi1484/knowledge-sanitization},
  preview={paper_2023b.jpg}
}

@article{ishibashi-etal-2023-evaluating,
  title={Evaluating the Robustness of Discrete Prompts},
  author       = {Ishibashi, Yoichi  and
      Bollegala, Danushka  and
      Sudoh, Katsuhito  and
      Nakamura, Satoshi},
  journal      = {EACL},
  volume       = {abs/2302.05619},
  year         = {2023},
  abstract = "Discrete prompts have been used for fine-tuning Pre-trained Language Models for diverse NLP tasks. In particular, automatic methods that generate discrete prompts from a small set of training instances have reported superior performance. However, a closer look at the learnt prompts reveals that they contain noisy and counter-intuitive lexical constructs that would not be encountered in manually-written prompts. This raises an important yet understudied question regarding the robustness of automatically learnt discrete prompts when used in downstream tasks. To address this question, we conduct a systematic study of the robustness of discrete prompts by applying carefully designed perturbations into an application using AutoPrompt and then measure their performance in two Natural Language Inference (NLI) datasets. Our experimental results show that although the discrete prompt-based method remains relatively robust against perturbations to NLI inputs, they are highly sensitive to other types of perturbations such as shuffling and deletion of prompt tokens. Moreover, they generalize poorly across different NLI datasets. We hope our findings will inspire future work on robust discrete prompt learning.",
  pdf = {https://arxiv.org/abs/2302.05619},
  preview={paper_2023a.jpg},
  abbr={EACL}
}



@article{DBLP:journals/corr/abs-2210-13034,
  author       = {Yoichi Ishibashi and
                  Sho Yokoi and
                  Katsuhito Sudoh and
                  Satoshi Nakamura},
  title        = {Subspace Representations for Soft Set Operations and Sentence Similarities},
  journal      = {NAACL},
  volume       = {abs/2210.13034},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2210.13034},
  doi          = {10.48550/ARXIV.2210.13034},
  eprinttype    = {arXiv},
  eprint       = {2210.13034},
  timestamp    = {Fri, 28 Oct 2022 14:21:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2210-13034.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In the field of natural language processing (NLP), continuous vector representations are crucial for capturing the semantic meanings of individual words. Yet, when it comes to the representations of sets of words, the conventional vector-based approaches often struggle with expressiveness and lack the essential set operations such as union, intersection, and complement. Inspired by quantum logic, we realize the representation of word sets and corresponding set operations within pre-trained word embedding spaces. By grounding our approach in the linear subspaces, we enable efficient computation of various set operations and facilitate the soft computation of membership functions within continuous spaces. Moreover, we allow for the computation of the F-score directly within word vectors, thereby establishing a direct link to the assessment of sentence similarity. In experiments with widely-used pre-trained embeddings and benchmarks, we show that our subspace-based set operations consistently outperform vector-based ones in both sentence similarity and set retrieval tasks. },
  pdf = {https://arxiv.org/abs/2210.13034},
  code = {https://github.com/yoichi1484/subspace},
  preview={paper_2022a.jpg}
}


@inproceedings{takahashi-etal-2021-multilingual,
    title = "Multilingual Machine Translation Evaluation Metrics Fine-tuned on Pseudo-Negative Examples for {WMT} 2021 Metrics Task",
    author = "Takahashi, Kosuke  and
      Ishibashi, Yoichi  and
      Sudoh, Katsuhito  and
      Nakamura, Satoshi",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.113",
    pages = "1049--1052",
    abstract = "This paper describes our submission to the WMT2021 shared metrics task. Our metric is operative to segment-level and system-level translations. Our belief toward a better metric is to detect a significant error that cannot be missed in the real practice cases of evaluation. For that reason, we used pseudo-negative examples in which attributes of some words are transferred to the reversed attribute words, and we build evaluation models to handle such serious mistakes of translations. We fine-tune a multilingual largely pre-trained model on the provided corpus of past years{'} metric task and fine-tune again further on the synthetic negative examples that are derived from the same fine-tune corpus. From the evaluation results of the WMT21{'}s development corpus, fine-tuning on the pseudo-negatives using WMT15-17 and WMT18-20 metric corpus achieved a better Pearson{'}s correlation score than the one fine-tuned without negative examples. Our submitted models,hyp+src{\_}hyp+ref and hyp+src{\_}hyp+ref.negative, are the plain model using WMT18-20 and the one additionally fine-tuned on negative samples, respectively.",
    pdf = {https://aclanthology.org/2021.wmt-1.113}
}


@inproceedings{ishibashi-etal-2020-reflection,
    title = "Reflection-based Word Attribute Transfer",
    author = "Ishibashi, Yoichi  and
      Sudoh, Katsuhito  and
      Yoshino, Koichiro  and
      Nakamura, Satoshi",
    editor = "Rijhwani, Shruti  and
      Liu, Jiangming  and
      Wang, Yizhong  and
      Dror, Rotem",
    booktitle = "ACL 2020: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-srw.8",
    doi = "10.18653/v1/2020.acl-srw.8",
    pages = "51--58",
    abstract = "Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word{'}s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes.",
    pdf = {https://aclanthology.org/2020.acl-srw.8},
    code = {https://github.com/ahclab/reflection}
}

@article{IshibashiACM,
  author       = {Yoichi Ishibashi and
                  Hisashi Miyamori},
  title        = {Generating Responses based on Information Visually-Induced by Text Utterance},
  year         = {2018},
  pdf          = {https://yoichi1484.github.io/assets/pdf/shizuka/paper.pdf},
}

@inproceedings{DBLP:conf/ntcir/IshibashiSM17,
  author       = {Yoichi Ishibashi and
                  Sho Sugimoto and
                  Hisashi Miyamori},
  title        = {{KSU} Team's Dialogue System at the {NTCIR-13} Short Text Conversation
                  Task 2},
  booktitle    = {The 13th {NTCIR} Conference, Evaluation of Information Access Technologies,
                  National Center of Sciences, Tokyo, Japan, December 5-8, 2017},
  publisher    = {National Institute of Informatics {(NII)}},
  year         = {2017},
  pdf          = {https://research.nii.ac.jp/ntcir/workshop/OnlineProceedings13/pdf/ntcir/10-NTCIR13-STC-IshibashiY.pdf},
  timestamp    = {Wed, 01 Jun 2022 17:01:01 +0200},
  biburl       = {https://dblp.org/rec/conf/ntcir/IshibashiSM17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
