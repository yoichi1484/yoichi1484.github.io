---
---

@article{ishibashi2025mining,
  title={Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning},
  author={Ishibashi, Yoichi and Yano, Taro and Oyamada, Masafumi},
  journal      = {arXiv},
  volume       = {abs/2505.10182},
  year         = {2025},
  pdf          = {https://arxiv.org/abs/2404.02183},
  preview={paper_2025a.jpg},
  selected={true}
}

@article{ishibashi2024can,
  title={Can Large Language Models Invent Algorithms to Improve Themselves?},
  author       = {Yoichi Ishibashi and
                  Taro Yano and
                  Masafumi Oyamada},
  journal      = {NAACL},
  volume       = {abs/2410.15639},
  year         = {2025},
  pdf          = {https://arxiv.org/abs/2410.15639},
  preview={paper_2024b.jpg},
  selected={true}
}

@article{ishibashi2024self,
  title={Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization},
  author       = {Yoichi Ishibashi and
                  Nishimura Yoshimasa},
  journal      = {arXiv},
  volume       = {abs/2404.02183},
  year         = {2024},
  pdf          = {https://arxiv.org/abs/2404.02183},
  code = {https://github.com/tsukushiAI/self-organized-agent},
  preview={paper_2024a.jpg},
  selected={true}
}


@article{DBLP:journals/corr/abs-2309-11852,
  author       = {Yoichi Ishibashi and
                  Hidetoshi Shimodaira},
  title        = {Knowledge Sanitization of Large Language Models},
  journal      = {arXiv},
  volume       = {abs/2309.11852},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.11852},
  doi          = {10.48550/ARXIV.2309.11852},
  eprinttype    = {arXiv},
  eprint       = {2309.11852},
  timestamp    = {Mon, 25 Sep 2023 15:34:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-11852.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  pdf = {https://arxiv.org/abs/2309.11852},
  code = {https://github.com/yoichi1484/knowledge-sanitization},
  preview={paper_2023b.jpg},
  selected={true}
}

@inproceedings{ishibashi-etal-2023-evaluating,
    title = "Evaluating the Robustness of Discrete Prompts",
    author = "Ishibashi, Yoichi  and
      Bollegala, Danushka  and
      Sudoh, Katsuhito  and
      Nakamura, Satoshi",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.174",
    doi = "10.18653/v1/2023.eacl-main.174",
    pages = "2373--2384",
    abstract = "Discrete prompts have been used for fine-tuning Pre-trained Language Models for diverse NLP tasks. In particular, automatic methods that generate discrete prompts from a small set of training instances have reported superior performance. However, a closer look at the learnt prompts reveals that they contain noisy and counter-intuitive lexical constructs that would not be encountered in manually-written prompts. This raises an important yet understudied question regarding the robustness of automatically learnt discrete prompts when used in downstream tasks. To address this question, we conduct a systematic study of the robustness of discrete prompts by applying carefully designed perturbations into an application using AutoPrompt and then measure their performance in two Natural Language Inference (NLI) datasets. Our experimental results show that although the discrete prompt-based method remains relatively robust against perturbations to NLI inputs, they are highly sensitive to other types of perturbations such as shuffling and deletion of prompt tokens. Moreover, they generalize poorly across different NLI datasets. We hope our findings will inspire future work on robust discrete prompt learning.",
    pdf = {https://arxiv.org/abs/2302.05619},
    code = {https://github.com/LivNLP/prompt-robustness},
    preview={paper_2023a.jpg},
  selected={false}

}


@article{DBLP:journals/corr/abs-2210-13034,
  author       = {Yoichi Ishibashi and
                  Sho Yokoi and
                  Katsuhito Sudoh and
                  Satoshi Nakamura},
  title        = {Subspace Representations for Soft Set Operations and Sentence Similarities},
  journal      = {NAACL},
  volume       = {abs/2210.13034},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2210.13034},
  doi          = {10.48550/ARXIV.2210.13034},
  eprinttype    = {arXiv},
  eprint       = {2210.13034},
  timestamp    = {Fri, 28 Oct 2022 14:21:57 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2210-13034.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract = {In the field of natural language processing (NLP), continuous vector representations are crucial for capturing the semantic meanings of individual words. Yet, when it comes to the representations of sets of words, the conventional vector-based approaches often struggle with expressiveness and lack the essential set operations such as union, intersection, and complement. Inspired by quantum logic, we realize the representation of word sets and corresponding set operations within pre-trained word embedding spaces. By grounding our approach in the linear subspaces, we enable efficient computation of various set operations and facilitate the soft computation of membership functions within continuous spaces. Moreover, we allow for the computation of the F-score directly within word vectors, thereby establishing a direct link to the assessment of sentence similarity. In experiments with widely-used pre-trained embeddings and benchmarks, we show that our subspace-based set operations consistently outperform vector-based ones in both sentence similarity and set retrieval tasks. },
  pdf = {https://arxiv.org/abs/2210.13034},
  code = {https://github.com/yoichi1484/subspace},
  preview={paper_2022a.jpg}
}


@inproceedings{takahashi-etal-2021-multilingual,
    title = "Multilingual Machine Translation Evaluation Metrics Fine-tuned on Pseudo-Negative Examples for {WMT} 2021 Metrics Task",
    author = "Takahashi, Kosuke  and
      Ishibashi, Yoichi  and
      Sudoh, Katsuhito  and
      Nakamura, Satoshi",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.113",
    pages = "1049--1052",
    abstract = "This paper describes our submission to the WMT2021 shared metrics task. Our metric is operative to segment-level and system-level translations. Our belief toward a better metric is to detect a significant error that cannot be missed in the real practice cases of evaluation. For that reason, we used pseudo-negative examples in which attributes of some words are transferred to the reversed attribute words, and we build evaluation models to handle such serious mistakes of translations. We fine-tune a multilingual largely pre-trained model on the provided corpus of past years{'} metric task and fine-tune again further on the synthetic negative examples that are derived from the same fine-tune corpus. From the evaluation results of the WMT21{'}s development corpus, fine-tuning on the pseudo-negatives using WMT15-17 and WMT18-20 metric corpus achieved a better Pearson{'}s correlation score than the one fine-tuned without negative examples. Our submitted models,hyp+src{\_}hyp+ref and hyp+src{\_}hyp+ref.negative, are the plain model using WMT18-20 and the one additionally fine-tuned on negative samples, respectively.",
    pdf = {https://aclanthology.org/2021.wmt-1.113}
}


@inproceedings{ishibashi-etal-2020-reflection,
    title = "Reflection-based Word Attribute Transfer",
    author = "Ishibashi, Yoichi  and
      Sudoh, Katsuhito  and
      Yoshino, Koichiro  and
      Nakamura, Satoshi",
    editor = "Rijhwani, Shruti  and
      Liu, Jiangming  and
      Wang, Yizhong  and
      Dror, Rotem",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-srw.8",
    doi = "10.18653/v1/2020.acl-srw.8",
    pages = "51--58",
    abstract = "Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word{'}s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes.",
    pdf = {https://aclanthology.org/2020.acl-srw.8},
    code = {https://github.com/ahclab/reflection}
}

@article{IshibashiACM,
  author       = {Yoichi Ishibashi and
                  Hisashi Miyamori},
  title        = {Generating Responses based on Information Visually-Induced by Text Utterance},
  year         = {2018},
  pdf          = {https://yoichi1484.github.io/projects/shizuka/doc/INLG2019/paper/paper_final2019.pdf},
}

@inproceedings{DBLP:conf/ntcir/IshibashiSM17,
  author       = {Yoichi Ishibashi and
                  Sho Sugimoto and
                  Hisashi Miyamori},
  title        = {{KSU} Team's Dialogue System at the {NTCIR-13} Short Text Conversation
                  Task 2},
  booktitle    = {The 13th {NTCIR} Conference, Evaluation of Information Access Technologies,
                  National Center of Sciences, Tokyo, Japan, December 5-8, 2017},
  publisher    = {National Institute of Informatics {(NII)}},
  year         = {2017},
  pdf          = {https://research.nii.ac.jp/ntcir/workshop/OnlineProceedings13/pdf/ntcir/10-NTCIR13-STC-IshibashiY.pdf},
  timestamp    = {Wed, 01 Jun 2022 17:01:01 +0200},
  biburl       = {https://dblp.org/rec/conf/ntcir/IshibashiSM17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
